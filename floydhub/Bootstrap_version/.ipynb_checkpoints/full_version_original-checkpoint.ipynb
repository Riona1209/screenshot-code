{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/fchollet/keras.git && cd keras && python setup.py install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.layers import Embedding,GRU,TimeDistributed,RepeatVector, LSTM, concatenate , Input, Reshape\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import backend as K \n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input, Dense, Dropout, RepeatVector, LSTM, concatenate, Conv2D, MaxPooling2D, Flatten\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_name = '/data/train_set/'\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_data(data_dir):\n",
    "    text = []\n",
    "    images = []\n",
    "    all_filenames = os.listdir(data_dir)\n",
    "    all_filenames.sort()\n",
    "    for filename in tqdm(all_filenames):\n",
    "        if filename[-3:] == \"npz\":\n",
    "            image = np.load(data_dir+filename)\n",
    "            images.append(image['features'])\n",
    "        else:\n",
    "            syntax = '<START> ' + load_doc(data_dir+filename) + ' <END>'\n",
    "            syntax = ' '.join(syntax.split())\n",
    "            syntax = syntax.replace(',', ' ,')\n",
    "            text.append(syntax)\n",
    "    images = np.array(images, dtype=float)\n",
    "    return images, text\n",
    "\n",
    "train_features, texts = load_data(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = 17\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, filters='', split=\" \", lower=False)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "train_sequences = tokenizer.texts_to_sequences(texts)\n",
    "max_sequence = max(len(s) for s in train_sequences)\n",
    "max_length = 48\n",
    "\n",
    "def preprocess_data(sequences, features):\n",
    "    X, y, image_data = list(), list(), list()\n",
    "    for img_no, seq in enumerate(sequences):\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_sequence)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            image_data.append(features[img_no])\n",
    "            X.append(in_seq[-48:])\n",
    "            y.append(out_seq)\n",
    "    return np.array(X), np.array(y), np.array(image_data)\n",
    "\n",
    "X, y, image_data = preprocess_data(train_sequences, train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_model = Sequential()\n",
    "image_model.add(Conv2D(32, (3, 3), padding='valid', activation='relu', input_shape=(299, 299, 3,)))\n",
    "image_model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))\n",
    "image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "image_model.add(Dropout(0.25))\n",
    "\n",
    "image_model.add(Conv2D(64, (3, 3), padding='valid', activation='relu'))\n",
    "image_model.add(Conv2D(64, (3, 3), padding='valid', activation='relu'))\n",
    "image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "image_model.add(Dropout(0.25))\n",
    "\n",
    "image_model.add(Conv2D(128, (3, 3), padding='valid', activation='relu'))\n",
    "image_model.add(Conv2D(128, (3, 3), padding='valid', activation='relu'))\n",
    "image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "image_model.add(Dropout(0.25))\n",
    "\n",
    "image_model.add(Flatten())\n",
    "image_model.add(Dense(1024, activation='relu'))\n",
    "image_model.add(Dropout(0.3))\n",
    "image_model.add(Dense(1024, activation='relu'))\n",
    "image_model.add(Dropout(0.3))\n",
    "\n",
    "image_model.add(RepeatVector(max_length))\n",
    "\n",
    "visual_input = Input(shape=(299, 299, 3,))\n",
    "encoded_image = image_model(visual_input)\n",
    "\n",
    "language_input = Input(shape=(max_length,))\n",
    "language_model = Embedding(vocab_size, 50, input_length=max_length, mask_zero=True)(language_input)\n",
    "language_model = LSTM(128, return_sequences=True)(language_model)\n",
    "language_model = LSTM(128, return_sequences=True)(language_model)\n",
    "\n",
    "decoder = concatenate([encoded_image, language_model])\n",
    "\n",
    "decoder = LSTM(512, return_sequences=True)(decoder)\n",
    "decoder = LSTM(512, return_sequences=False)(decoder)\n",
    "decoder = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "model = Model(inputs=[visual_input, language_input], outputs=decoder)\n",
    "\n",
    "optimizer = RMSprop(lr=0.0001, clipvalue=1.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train model\n",
    "filepath=\"org-weights-epoch-{epoch:04d}--val_loss-{val_loss:.4f}--loss-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=True, period=1)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([image_data, X], y, batch_size=64, shuffle=False, validation_split=0.1, callbacks=callbacks_list, verbose=1, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
